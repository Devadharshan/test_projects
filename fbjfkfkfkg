#!/usr/bin/env python3
import os
import re
import glob
import logging
from datetime import datetime
from collections import defaultdict
from prometheus_client import CollectorRegistry, Gauge, pushadd_to_gateway

# =====================================================
# CONFIG
# =====================================================
LOG_DIR = r"\\windows-share\logs"   # CHANGE THIS
FILE_PATTERN = "ReportingTransfer_ProdLon_x64__*.log"
PUSHGATEWAY_URL = "http://localhost:9091"
JOB_NAME = "regulatory_reporting_processing"
INSTANCE_NAME = "prod_lon_reporting_logs"

# =====================================================
# LOGGING
# =====================================================
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("reg_reporting")

# =====================================================
# REGEX (FINAL & SAFE)
# =====================================================
START_PATTERN = re.compile(
    r"RegulatoryReporting(?:Importer|Exporter):\s*"
    r"New item to access queue:\s*"
    r"Message Type:\s*(?P<msg_type>[^,]+),\s*"
    r"Trade ID:\s*(?P<trade_id>\d+),\s*"
    r"Trade Version:\s*(?P<trade_ver>\d+)"
)

END_PATTERN = re.compile(
    r"RegulatoryReporting(?:Importer|Exporter):\s*"
    r"Successfully processed item:\s*"
    r"Message Type:\s*(?P<msg_type>[^,]+),\s*"
    r"Trade ID:\s*(?P<trade_id>\d+),\s*"
    r"Trade Version:\s*(?P<trade_ver>\d+)"
)

FLOW_PATTERN = re.compile(r"FlowID:\s*(\d+)")
TEMPLATE_PATTERN = re.compile(r"Sent Template:\s*([A-Za-z0-9\-_.]+)")

# =====================================================
# DATA STRUCTURES
# =====================================================
start_events = {}
durations = defaultdict(list)
counts = defaultdict(int)

# =====================================================
# HELPERS
# =====================================================
def parse_datetime(date_str, time_str):
    return datetime.strptime(
        f"{date_str} {time_str}", "%Y-%m-%d %H:%M:%S.%f"
    )

# =====================================================
# PROCESS FILES
# =====================================================
files = glob.glob(os.path.join(LOG_DIR, FILE_PATTERN))
logger.info(f"Found {len(files)} files")

for file_path in files:
    logger.info(f"Processing {file_path}")
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            parts = line.strip().split("|")
            if len(parts) < 7:
                continue

            log_date, log_time = parts[0], parts[1]
            message = parts[6]

            try:
                ts = parse_datetime(log_date, log_time)
            except ValueError:
                continue

            flow_match = FLOW_PATTERN.search(message)
            tmpl_match = TEMPLATE_PATTERN.search(message)

            flow_id = flow_match.group(1) if flow_match else "none"
            template = tmpl_match.group(1) if tmpl_match else "none"

            sm = START_PATTERN.search(message)
            if sm:
                key = (
                    sm.group("msg_type").strip(),
                    sm.group("trade_id"),
                    sm.group("trade_ver")
                )
                start_events[key] = (ts, log_date, flow_id, template)
                continue

            em = END_PATTERN.search(message)
            if em and (key := (
                em.group("msg_type").strip(),
                em.group("trade_id"),
                em.group("trade_ver")
            )) in start_events:

                start_ts, start_date, s_flow, s_tmpl = start_events.pop(key)
                duration = (ts - start_ts).total_seconds()

                durations[(
                    key[0],
                    key[1],
                    key[2],
                    start_date,
                    flow_id if flow_id != "none" else s_flow,
                    template if template != "none" else s_tmpl
                )].append(duration)

                counts[(key[0], start_date)] += 1

# =====================================================
# PROMETHEUS
# =====================================================
registry = CollectorRegistry()

Gauge(
    "reg_reporting_script_heartbeat",
    "Script heartbeat",
    ["instance"],
    registry=registry
).labels(instance=INSTANCE_NAME).set(1)

duration_gauge = Gauge(
    "reg_reporting_processing_seconds",
    "Processing duration",
    ["message_type", "trade_id", "trade_version",
     "flow_id", "sent_template", "log_date", "instance"],
    registry=registry
)

count_gauge = Gauge(
    "reg_reporting_message_count",
    "Processed message count",
    ["message_type", "log_date", "instance"],
    registry=registry
)

for (m, t, v, d, f, s), vals in durations.items():
    duration_gauge.labels(m, t, v, f, s, d, INSTANCE_NAME).set(sum(vals)/len(vals))

for (m, d), c in counts.items():
    count_gauge.labels(m, d, INSTANCE_NAME).set(c)

logger.info(f"Prepared {len(durations)} duration metrics")
logger.info(f"Prepared {len(counts)} count metrics")

pushadd_to_gateway(PUSHGATEWAY_URL, job=JOB_NAME, registry=registry)

logger.info("âœ… Metrics pushed successfully")