#!/usr/bin/env python3
"""
push_bp_trend_raw_fixed.py

- Same behaviour as your canonical script but avoids duplicate-gauge errors by
  creating a fresh CollectorRegistry for each push.
- Forces IST timezone for windows.
- Computes time_taken from Start/End columns (dayfirst parsing).
- Sends metrics to Pushgateway at http://localhost:9091
- Jobs:
    - bp_summary_raw
    - bp_non_summary_raw

Requires: pip install pandas prometheus_client
"""
import os
import re
import math
import pandas as pd
import traceback
from datetime import datetime, timedelta, timezone
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

# Python 3.9+: zoneinfo
try:
    from zoneinfo import ZoneInfo
    IST = ZoneInfo("Asia/Kolkata")
except Exception:
    IST = timezone(timedelta(hours=5, minutes=30))

# ---------------- CONFIG ----------------
DATA_FOLDER = r"C:\bpmetrics\data"   # <-- change this path
PUSHGATEWAY = "http://localhost:9091"

JOB_SUMMARY_RAW = "bp_summary_raw"
JOB_NON_SUMMARY_RAW = "bp_non_summary_raw"

# Candidate column name variants (kept for compatibility)
START_COL_CANDIDATES = ["start_time", "start time", "starttime", "start"]
END_COL_CANDIDATES = ["end_time", "end time", "endtime", "end"]
TRADES_COL_CANDIDATES = ["no_of_trades", "no of trades", "number_of_trades", "numberoftrades", "trades", "no_of_trade"]

# ---------------- HELPERS ----------------
def read_csv_safely(path):
    try:
        return pd.read_csv(path, dtype=str)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin-1", dtype=str)
    except Exception as e:
        print(f"[read_csv_safely] Error reading {path}: {e}")
        return pd.DataFrame()

def extract_date_from_filename(fname):
    m = re.findall(r"(\d{4}-\d{2}-\d{2})", fname)
    if not m:
        return None
    try:
        return datetime.strptime(m[-1], "%Y-%m-%d").date()
    except Exception:
        return None

def clean_label_key(k: str) -> str:
    if k is None:
        return "unknown"
    s = str(k).strip().lower()
    s = re.sub(r'[^a-z0-9_]', '_', s)
    if not re.match(r'^[a-z]', s):
        s = 'k_' + s
    return s

def clean_label_value(v: str) -> str:
    if v is None:
        return "unknown"
    s = str(v).strip()
    if s == "":
        return "unknown"
    return re.sub(r'[^A-Za-z0-9_:./@-]', '_', s)

def is_number_like(s):
    if s is None:
        return False
    s = str(s).strip()
    if s == "":
        return False
    s2 = s.replace(",", "")
    try:
        float(s2)
        return True
    except:
        return False

def to_float(s):
    if s is None:
        return None
    s = str(s).strip()
    if s == "" or s.lower() == "nan":
        return None
    try:
        return float(s.replace(",", ""))
    except:
        return None

def windows_for_date(d, today_date):
    windows = []
    if d is None:
        return windows
    yesterday = today_date - timedelta(days=1)
    if d == today_date:
        windows.append("today")
    if d == yesterday:
        windows.append("yesterday")
    if (today_date - timedelta(days=6)) <= d <= today_date:
        windows.append("last_7_days")
    if (today_date - timedelta(days=29)) <= d <= today_date:
        windows.append("last_30_days")
    return windows

def find_first_col(df, candidates):
    cols = list(df.columns)
    normalized = {}
    for c in cols:
        key = re.sub(r'[\s_]+', '', c.strip().lower())
        normalized[key] = c
    for cand in candidates:
        k = re.sub(r'[\s_]+', '', cand.strip().lower())
        if k in normalized:
            return normalized[k]
    return None

# ---------------- MAIN ----------------
def main():
    now_ist = datetime.now(IST)
    today_date = now_ist.date()

    try:
        files = sorted([f for f in os.listdir(DATA_FOLDER) if f.lower().endswith(".csv")])
    except Exception as e:
        print(f"[main] Could not list DATA_FOLDER {DATA_FOLDER}: {e}")
        return

    if not files:
        print(f"[main] No CSV files found in {DATA_FOLDER}")
        return

    summary_rows = []
    non_summary_rows = []

    for fname in files:
        full = os.path.join(DATA_FOLDER, fname)
        df = read_csv_safely(full)
        if df is None or df.empty:
            print(f"[main] skipping (empty/unreadable) {fname}")
            continue

        # preserve original column names (trim)
        df.columns = [c.strip() for c in df.columns]
        file_date = extract_date_from_filename(fname)
        df["file_name"] = fname
        df["file_date"] = file_date

        if "_summary" in fname.lower():
            summary_rows.append(df)
        else:
            non_summary_rows.append(df)

    summary_df = pd.concat(summary_rows, ignore_index=True) if summary_rows else pd.DataFrame()
    non_summary_df = pd.concat(non_summary_rows, ignore_index=True) if non_summary_rows else pd.DataFrame()

    # ---------------- Derive time_taken and durationpertrade for non-summary ----------------
    if not non_summary_df.empty:
        start_col = find_first_col(non_summary_df, START_COL_CANDIDATES)
        end_col = find_first_col(non_summary_df, END_COL_CANDIDATES)
        trades_col = find_first_col(non_summary_df, TRADES_COL_CANDIDATES)

        if start_col and end_col:
            try:
                s = pd.to_datetime(non_summary_df[start_col], dayfirst=True, errors="coerce")
                e = pd.to_datetime(non_summary_df[end_col], dayfirst=True, errors="coerce")
                non_summary_df["time_taken"] = (e - s).dt.total_seconds()
                non_summary_df["time_taken"] = non_summary_df["time_taken"].where(non_summary_df["time_taken"].notna(), None)
            except Exception as ex:
                print(f"[main] failed to compute time_taken: {ex}")
                non_summary_df["time_taken"] = None
        else:
            non_summary_df["time_taken"] = None

        if "time_taken" in non_summary_df.columns:
            non_summary_df["duration"] = non_summary_df["time_taken"]

        # compute trades numeric helper
        if trades_col:
            non_summary_df["_trades_num"] = non_summary_df[trades_col].apply(to_float)
        else:
            # try common names
            if "No of trades" in non_summary_df.columns:
                non_summary_df["_trades_num"] = non_summary_df["No of trades"].apply(to_float)
            else:
                non_summary_df["_trades_num"] = None

        def compute_dpt(row):
            try:
                dur_val = to_float(row.get("duration"))
                tr_val = to_float(row.get("_trades_num"))
                if dur_val is None or tr_val is None or tr_val == 0:
                    return None
                return dur_val / tr_val
            except Exception:
                return None

        non_summary_df["durationpertrade"] = non_summary_df.apply(compute_dpt, axis=1)
        if "_trades_num" in non_summary_df.columns:
            non_summary_df.drop(columns=["_trades_num"], inplace=True)

    # ---------------- Prepare & push (fresh registry per push) ----------------
    def prepare_gauges_for_df(df, prefix, registry):
        gauges = {}
        if df.empty:
            return gauges

        numeric_cols = []
        for col in df.columns:
            if col in ("file_name", "file_date"):
                continue
            key_lc = col.strip().lower()
            # avoid treating start/end as numeric columns
            if key_lc in [re.sub(r'[\s_]+', '', c) for c in START_COL_CANDIDATES + END_COL_CANDIDATES]:
                continue
            try:
                sample = df[col].dropna().astype(str)
                if any(is_number_like(x) for x in sample.head(50)):
                    numeric_cols.append(col)
            except Exception:
                continue

        all_label_cols = []
        time_cols_norm = set([re.sub(r'[\s_]+', '', c.strip().lower()) for c in START_COL_CANDIDATES + END_COL_CANDIDATES])
        for col in df.columns:
            if col in numeric_cols:
                continue
            if col in ("file_name", "file_date"):
                continue
            col_norm = re.sub(r'[\s_]+', '', col.strip().lower())
            if col_norm in time_cols_norm:
                continue
            all_label_cols.append(col)

        label_keys = [clean_label_key(c) for c in all_label_cols] + ["file_name", "file_date", "window"]

        for col in numeric_cols:
            metric_name = prefix + "_" + re.sub(r'[^a-z0-9_]', '_', col.strip().lower())
            if "duration" in col.lower() and not metric_name.endswith("_seconds"):
                metric_name = metric_name + "_seconds"
            try:
                gauge = Gauge(metric_name, f"{metric_name} (from csv column {col})", labelnames=label_keys, registry=registry)
                gauges[col] = (gauge, metric_name, label_keys)
            except Exception as e:
                # If creation fails, log and continue (prevents crash)
                print(f"[prepare_gauges_for_df] could not create gauge {metric_name}: {e}")
                continue
        return gauges

    def push_rows(df, gauges_map, registry, job_name):
        if df.empty or not gauges_map:
            print(f"[push_rows] nothing to push for job {job_name}")
            return

        numeric_cols = set(gauges_map.keys())

        set_count = 0
        for idx, row in df.iterrows():
            file_date_obj = row.get("file_date")
            windows = windows_for_date(file_date_obj, today_date)
            if not windows:
                continue

            label_items = {}
            for col in df.columns:
                if col in numeric_cols:
                    continue
                if col in ("file_date",):
                    continue
                col_norm = re.sub(r'[\s_]+', '', col.strip().lower())
                if col_norm in set([re.sub(r'[\s_]+', '', c.strip().lower()) for c in START_COL_CANDIDATES + END_COL_CANDIDATES]):
                    continue
                key = clean_label_key(col)
                val = clean_label_value(row.get(col, "unknown"))
                label_items[key] = val

            for w in windows:
                label_items_with_window = dict(label_items)
                label_items_with_window["file_name"] = clean_label_value(row.get("file_name", "unknown"))
                fd_str = row.get("file_date")
                if isinstance(fd_str, (datetime,)):
                    fd_str = fd_str.date().isoformat()
                elif fd_str is None:
                    fd_str = ""
                label_items_with_window["file_date"] = str(fd_str)
                label_items_with_window["window"] = w

                for col, (gauge_obj, metric_name, label_keys) in gauges_map.items():
                    value_raw = row.get(col)
                    value_to_set = to_float(value_raw) if value_raw is not None else 0.0
                    label_values = []
                    for lk in label_keys:
                        label_values.append(label_items_with_window.get(lk, "unknown"))
                    try:
                        gauge_obj.labels(*label_values).set(value_to_set if value_to_set is not None else 0.0)
                        set_count += 1
                    except Exception as e:
                        print(f"[push_rows] failed to set {metric_name} labels={label_values} val={value_to_set}: {e}")

        # push registry only if we set at least one gauge
        if set_count > 0:
            try:
                push_to_gateway(PUSHGATEWAY, job=job_name, registry=registry)
                print(f"Pushed {set_count} samples -> job: {job_name}")
            except Exception as e:
                print(f"[push_rows] push_to_gateway failed for job {job_name}: {e}\n{traceback.format_exc()}")
        else:
            print(f"[push_rows] no samples were set for job {job_name}; skipping push.")

    # ----- Summary push (fresh registry) -----
    summary_registry = CollectorRegistry()
    summary_gauges = prepare_gauges_for_df(summary_df, "new_data_summary_row", summary_registry)
    push_rows(summary_df, summary_gauges, summary_registry, JOB_SUMMARY_RAW)

    # ----- Non-summary push (fresh registry) -----
    non_summary_registry = CollectorRegistry()
    non_summary_gauges = prepare_gauges_for_df(non_summary_df, "new_data_non_summary_row", non_summary_registry)
    push_rows(non_summary_df, non_summary_gauges, non_summary_registry, JOB_NON_SUMMARY_RAW)

    print("Done. Check Pushgateway:", PUSHGATEWAY)

if __name__ == "__main__":
    main()