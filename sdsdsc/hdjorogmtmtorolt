#!/usr/bin/env python3
"""
push_bp_trend_raw.py (updated)

- Trend-only (no aggregations) raw row exporter.
- Forces IST timezone for 'today', 'yesterday', 'last_7_days', 'last_30_days'.
- Computes time_taken from Start/End columns (dayfirst parsing).
- Sends metrics to Pushgateway at http://localhost:9091
- Jobs:
    - bp_summary_raw
    - bp_non_summary_raw

Edit DATA_FOLDER to your CSV directory before running.
Requires: pip install pandas prometheus_client
"""
import os
import re
import math
import pandas as pd
from datetime import datetime, timedelta, timezone
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

# Python 3.9+: zoneinfo
try:
    from zoneinfo import ZoneInfo
    IST = ZoneInfo("Asia/Kolkata")
except Exception:
    # fallback: create fixed offset tz for IST
    IST = timezone(timedelta(hours=5, minutes=30))

# ---------------- CONFIG ----------------
DATA_FOLDER = r"C:\bpmetrics\data"   # <-- change to your folder
PUSHGATEWAY = "http://localhost:9091"

JOB_SUMMARY_RAW = "bp_summary_raw"
JOB_NON_SUMMARY_RAW = "bp_non_summary_raw"

# Candidate column name variants for start/end/trades
START_COL_CANDIDATES = ["start_time", "start time", "starttime", "start"]
END_COL_CANDIDATES = ["end_time", "end time", "endtime", "end"]
TRADES_COL_CANDIDATES = ["no_of_trades", "no of trades", "number_of_trades", "numberoftrades", "trades", "no_of_trade"]

# ---------------- HELPERS ----------------
def read_csv_safely(path):
    try:
        return pd.read_csv(path, dtype=str)  # read as strings first
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin-1", dtype=str)
    except Exception as e:
        print(f"[read_csv_safely] Error reading {path}: {e}")
        return pd.DataFrame()

def extract_date_from_filename(fname):
    m = re.findall(r"(\d{4}-\d{2}-\d{2})", fname)
    if not m:
        return None
    try:
        return datetime.strptime(m[-1], "%Y-%m-%d").date()
    except Exception:
        return None

def clean_label_key(k: str) -> str:
    """Normalize CSV header -> prometheus label key"""
    if k is None:
        return "unknown"
    s = str(k).strip().lower()
    s = re.sub(r'[^a-z0-9_]', '_', s)
    if not re.match(r'^[a-z]', s):
        s = 'k_' + s
    return s

def clean_label_value(v: str) -> str:
    if v is None:
        return "unknown"
    s = str(v).strip()
    if s == "":
        return "unknown"
    # replace problematic chars
    return re.sub(r'[^A-Za-z0-9_:./@-]', '_', s)

def is_number_like(s):
    if s is None:
        return False
    s = str(s).strip()
    if s == "":
        return False
    # allow ints/floats with commas
    s2 = s.replace(",", "")
    try:
        float(s2)
        return True
    except:
        return False

def to_float(s):
    if s is None:
        return None
    s = str(s).strip()
    if s == "" or s.lower() == "nan":
        return None
    try:
        return float(s.replace(",", ""))
    except:
        return None

def windows_for_date(d, today_date):
    """Return list of window labels this date belongs to (today, yesterday, last_7_days, last_30_days)."""
    windows = []
    if d is None:
        return windows
    yesterday = today_date - timedelta(days=1)
    if d == today_date:
        windows.append("today")
    if d == yesterday:
        windows.append("yesterday")
    if (today_date - timedelta(days=6)) <= d <= today_date:
        windows.append("last_7_days")
    if (today_date - timedelta(days=29)) <= d <= today_date:
        windows.append("last_30_days")
    return windows

def find_first_col(df, candidates):
    """Return actual dataframe column name matching any candidate (case-insensitive, ignore underscores/spaces)."""
    cols = list(df.columns)
    normalized = {}
    for c in cols:
        key = re.sub(r'[\s_]+', '', c.strip().lower())
        normalized[key] = c
    for cand in candidates:
        k = re.sub(r'[\s_]+', '', cand.strip().lower())
        if k in normalized:
            return normalized[k]
    return None

# ---------------- MAIN ----------------
def main():
    # IST-aware today
    now_ist = datetime.now(IST)
    today_date = now_ist.date()

    # read files
    files = sorted([f for f in os.listdir(DATA_FOLDER) if f.lower().endswith(".csv")])

    if not files:
        print(f"[main] No CSV files found in {DATA_FOLDER}")
        return

    # collect rows
    summary_rows = []
    non_summary_rows = []

    for fname in files:
        full = os.path.join(DATA_FOLDER, fname)
        df = read_csv_safely(full)
        if df is None or df.empty:
            print(f"[main] skipping (empty/unreadable) {fname}")
            continue

        # normalize header keys to safe python strings (preserve original casing/spacing for lookup)
        original_columns = list(df.columns)
        norm_cols = [col.strip() for col in original_columns]
        df.columns = norm_cols

        file_date = extract_date_from_filename(fname)
        df["file_name"] = fname
        df["file_date"] = file_date

        # classify
        if "_summary" in fname.lower():
            summary_rows.append(df)
        else:
            non_summary_rows.append(df)

    # concat
    summary_df = pd.concat(summary_rows, ignore_index=True) if summary_rows else pd.DataFrame()
    non_summary_df = pd.concat(non_summary_rows, ignore_index=True) if non_summary_rows else pd.DataFrame()

    # ---------------- Derive time_taken and durationpertrade for non-summary ----------------
    if not non_summary_df.empty:
        # find start/end columns
        start_col = find_first_col(non_summary_df, START_COL_CANDIDATES)
        end_col = find_first_col(non_summary_df, END_COL_CANDIDATES)
        trades_col = find_first_col(non_summary_df, TRADES_COL_CANDIDATES)

        # parse to datetime (dayfirst=True because format is dd/mm/yyyy H:MM)
        if start_col and end_col:
            try:
                s = pd.to_datetime(non_summary_df[start_col], dayfirst=True, errors="coerce")
                e = pd.to_datetime(non_summary_df[end_col], dayfirst=True, errors="coerce")
                non_summary_df["time_taken"] = (e - s).dt.total_seconds()
                # any negative/NaN -> set to NaN then later fill 0
                non_summary_df["time_taken"] = non_summary_df["time_taken"].where(non_summary_df["time_taken"].notna(), None)
            except Exception as ex:
                print(f"[main] failed to compute time_taken: {ex}")
                non_summary_df["time_taken"] = None
        else:
            # no start/end columns found
            non_summary_df["time_taken"] = None

        # overwrite/define duration column to use computed time_taken (if computed)
        # prefer computed time_taken; fallback to existing 'duration' column if present
        if "time_taken" in non_summary_df.columns:
            # convert values to float where possible
            non_summary_df["duration"] = non_summary_df["time_taken"]
        else:
            # keep existing duration column as-is
            pass

        # compute durationpertrade if not present or to overwrite based on computed time
        # find trades count column name to compute numeric trades
        if trades_col:
            # create numeric trades series
            non_summary_df["_trades_num"] = non_summary_df[trades_col].apply(to_float)
        else:
            # attempt common names
            non_summary_df["_trades_num"] = non_summary_df.get("no_of_trades")  # may be None
            non_summary_df["_trades_num"] = non_summary_df["_trades_num"].apply(lambda x: to_float(x) if x is not None else None)

        # compute durationpertrade
        def compute_dpt(row):
            try:
                dur = row.get("duration")
                tr = row.get("_trades_num")
                dur_val = to_float(dur)
                tr_val = to_float(tr)
                if dur_val is None or tr_val is None or tr_val == 0:
                    return None
                return dur_val / tr_val
            except Exception:
                return None

        non_summary_df["durationpertrade"] = non_summary_df.apply(compute_dpt, axis=1)

        # clean helper column
        # keep _trades_num for possible use in labels? better drop it to avoid it becoming a label
        if "_trades_num" in non_summary_df.columns:
            non_summary_df.drop(columns=["_trades_num"], inplace=True)

    # ---------------- prepare push registries and gauges ----------------
    summary_registry = CollectorRegistry()
    non_summary_registry = CollectorRegistry()

    # Helper to prepare gauges dict for given dataframe
    def prepare_gauges_for_df(df, prefix, registry):
        """
        Determine numeric columns and create a Gauge per numeric column.
        Also determine non-numeric columns to be used as labels (excluding time columns).
        """
        gauges = {}
        if df.empty:
            return gauges

        # Determine numeric columns (treat any column where at least one value looks numeric)
        numeric_cols = []
        for col in df.columns:
            # skip file_name and file_date and other system columns if present in df
            if col in ("file_name", "file_date"):
                continue
            # skip obvious time columns so they don't become numeric even if parseable
            key_lc = col.strip().lower()
            if key_lc in [c.replace('_', '') for c in START_COL_CANDIDATES + END_COL_CANDIDATES]:
                continue
            # check if any value in column is number-like
            try:
                sample = df[col].dropna().astype(str)
                if any(is_number_like(x) for x in sample.head(50)):  # quick heuristic
                    numeric_cols.append(col)
            except Exception:
                continue

        # Label keys: take all columns that are not numeric, and exclude start/end/time columns
        all_label_cols = []
        time_cols_norm = set([re.sub(r'[\s_]+', '', c.strip().lower()) for c in START_COL_CANDIDATES + END_COL_CANDIDATES])
        for col in df.columns:
            if col in numeric_cols:
                continue
            if col in ("file_name", "file_date"):
                continue
            # skip raw start/end time columns (we avoid adding them as labels)
            col_norm = re.sub(r'[\s_]+', '', col.strip().lower())
            if col_norm in time_cols_norm:
                continue
            all_label_cols.append(col)

        # normalize label keys to safe prometheus labels, add file_name,file_date,window
        label_keys = [clean_label_key(c) for c in all_label_cols] + ["file_name", "file_date", "window"]

        # create gauge per numeric column
        for col in numeric_cols:
            metric_name = prefix + "_" + re.sub(r'[^a-z0-9_]', '_', col.strip().lower())
            # ensure metric name ends with suffix if it's a duration-like field
            if "duration" in col.lower() and not metric_name.endswith("_seconds"):
                metric_name = metric_name + "_seconds"
            # Create gauge
            try:
                gauge = Gauge(metric_name, f"{metric_name} (from csv column {col})", labelnames=label_keys, registry=registry)
                gauges[col] = (gauge, metric_name, label_keys)
            except ValueError as ve:
                # possible duplicate gauge name; skip or continue
                print(f"[prepare_gauges_for_df] could not create gauge {metric_name}: {ve}")
                continue
        return gauges

    # Prepare gauges
    summary_gauges = prepare_gauges_for_df(summary_df, "new_data_summary_row", summary_registry)
    non_summary_gauges = prepare_gauges_for_df(non_summary_df, "new_data_non_summary_row", non_summary_registry)

    # Function to push rows for a df using prepared gauges
    def push_rows(df, gauges_map):
        """
        df: dataframe with original column names
        gauges_map: mapping col_name -> (Gauge, metric_name, label_keys)
        """
        if df.empty or not gauges_map:
            return

        numeric_cols = set(gauges_map.keys())

        for idx, row in df.iterrows():
            file_date_obj = row.get("file_date")
            windows = windows_for_date(file_date_obj, today_date)
            if not windows:
                continue

            # build label base: all non-numeric columns as labels
            label_items = {}
            for col in df.columns:
                if col in numeric_cols:
                    continue
                if col in ("file_date",):  # keep file_date handled separately
                    continue
                # skip start/end raw columns (we don't want them as labels)
                col_norm = re.sub(r'[\s_]+', '', col.strip().lower())
                if col_norm in set([re.sub(r'[\s_]+', '', c.strip().lower()) for c in START_COL_CANDIDATES + END_COL_CANDIDATES]):
                    continue
                key = clean_label_key(col)
                val = clean_label_value(row.get(col, "unknown"))
                label_items[key] = val

            for w in windows:
                # create label_items_with_window copy
                label_items_with_window = dict(label_items)
                label_items_with_window["file_name"] = clean_label_value(row.get("file_name", "unknown"))
                fd_str = row.get("file_date")
                if isinstance(fd_str, (datetime,)):
                    fd_str = fd_str.date().isoformat()
                elif fd_str is None:
                    fd_str = ""
                label_items_with_window["file_date"] = str(fd_str)
                label_items_with_window["window"] = w

                # Set all numeric metrics for this row & window
                for col, (gauge_obj, metric_name, label_keys) in gauges_map.items():
                    # if metric is computed and present, use computed; else pull raw
                    value_raw = row.get(col)
                    value_to_set = to_float(value_raw) if value_raw is not None else 0.0
                    # build label values in exact order
                    label_values = []
                    for lk in label_keys:
                        if lk in label_items_with_window:
                            label_values.append(label_items_with_window[lk])
                        else:
                            label_values.append("unknown")
                    try:
                        gauge_obj.labels(*label_values).set(value_to_set if value_to_set is not None else 0.0)
                    except Exception as e:
                        print(f"[push_rows] failed to set {metric_name} labels={label_values} val={value_to_set}: {e}")

    # Push summary rows (job bp_summary_raw)
    if not summary_df.empty and summary_gauges:
        push_rows(summary_df, summary_gauges)
        try:
            push_to_gateway(PUSHGATEWAY, job=JOB_SUMMARY_RAW, registry=summary_registry)
            print(f"Pushed summary raw -> job: {JOB_SUMMARY_RAW}")
        except Exception as e:
            print(f"[ERROR] push summary to gateway: {e}")
    else:
        print("No summary rows or no numeric columns detected to push.")

    # Push non-summary rows (job bp_non_summary_raw)
    if not non_summary_df.empty and non_summary_gauges:
        push_rows(non_summary_df, non_summary_gauges)
        try:
            push_to_gateway(PUSHGATEWAY, job=JOB_NON_SUMMARY_RAW, registry=non_summary_registry)
            print(f"Pushed non-summary raw -> job: {JOB_NON_SUMMARY_RAW}")
        except Exception as e:
            print(f"[ERROR] push non-summary to gateway: {e}")
    else:
        print("No non-summary rows or no numeric columns detected to push.")

    print("Done. Check Pushgateway:", PUSHGATEWAY)

if __name__ == "__main__":
    main()