To monitor the RAG (Red, Amber, Green) status of a critical service based on volume, availability, latency, and errors using Prometheus, Loki, and traces, you can consider the following metrics and logs:


---

1. Volume Metrics

Goal: Understand the throughput (request or transaction volume).

Prometheus:

Metric: http_requests_total or a similar custom metric.

Consider thresholds based on normal traffic patterns (e.g., spikes or drops).

Thresholds:

Green: Normal volume.

Amber: Slightly higher or lower than expected range.

Red: Sudden drop to 0 (indicative of service downtime) or extreme spike.



Loki:

Look for unusual patterns in logs (e.g., very high or very low log generation frequency).


Traces:

High frequency of spans or no spans being generated can indicate anomalies.




---

2. Availability

Goal: Ensure the service is up and responding.

Prometheus:

Metric: up{job="your_service"} or custom health-check metric.

Query: up{job="your_service"} == 0 indicates downtime.

Thresholds:

Green: up is 1.

Amber: Intermittent drops in up.

Red: Persistent up == 0.



Loki:

Search for error logs like Connection timeout, 503, or Service unavailable.


Traces:

Missing traces for a period could indicate service unavailability.




---

3. Latency

Goal: Measure response times for requests.

Prometheus:

Metric: http_request_duration_seconds or custom latency metric.

Use percentiles (e.g., 95th percentile latency):

Query: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))


Thresholds (example):

Green: < 300ms.

Amber: 300ms - 1s.

Red: > 1s.



Loki:

Parse logs for response times if available (e.g., "Response time: 1200ms").


Traces:

Aggregate trace durations for critical operations and set thresholds.




---

4. Errors

Goal: Identify failures in the service.

Prometheus:

Metric: http_errors_total, 5xx_count, or similar.

Query: rate(http_errors_total[5m]) > threshold

Thresholds:

Green: No errors or very low error rate.

Amber: Noticeable increase in error rate.

Red: Sustained high error rate (e.g., >5% of requests).



Loki:

Search logs for error patterns:

Exception stack traces.

Specific keywords like "error," "failed," or "timeout."



Traces:

Identify spans with error=true tags and aggregate them.




---

5. Combining Data for RAG Status

To calculate the RAG status, you can define thresholds for all metrics and combine them:

1. Prometheus Rules:

Create Prometheus alerting rules to evaluate combined metrics.

Example:

ALERT ServiceRed
IF (
  (up{job="your_service"} == 0) OR 
  (rate(http_errors_total[5m]) > 0.05) OR
  (histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1)
)



2. Grafana Dashboard:

Visualize thresholds with color-coded panels for each metric.

Combine metrics using expressions to show an aggregated RAG status.



3. Loki Queries:

Use logs to validate issues during alert investigations.



4. Trace Analysis:

Integrate tracing tools like Jaeger or Tempo with Prometheus and Grafana for deeper root cause analysis.





---

Example RAG Thresholds


---

Would you like help setting up Prometheus alerting rules, Grafana panels, or Loki queries?

