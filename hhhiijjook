#!/usr/bin/env python3
import os
import re
import glob
import logging
from datetime import datetime
from collections import defaultdict

from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

# =====================================================
# CONFIGURATION
# =====================================================
LOG_DIR = r"\\windows-share\logs"   # <-- CHANGE THIS
FILE_PATTERN = "ReportingTransfer_ProdLon_x64__*.log"

PUSHGATEWAY_URL = "http://localhost:9091"
JOB_NAME = "regulatory_reporting_processing"

# =====================================================
# LOGGING
# =====================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
logger = logging.getLogger("reg_reporting")

# =====================================================
# REGEX PATTERNS
# =====================================================
START_PATTERN = re.compile(
    r"New item to access queue:\s*Message Type:\s*(?P<msg_type>[^,]+),\s*"
    r"Trade Id:(?P<trade_id>\d+),\s*Trade Version:(?P<trade_ver>\d+)"
)

END_PATTERN = re.compile(
    r"Successfully processed item:\s*Message Type:\s*(?P<msg_type>[^,]+),\s*"
    r"Trade Id:\s*(?P<trade_id>\d+),\s*Trade Version:\s*(?P<trade_ver>\d+)"
)

# =====================================================
# DATA STRUCTURES
# =====================================================
# key = (message_type, trade_id, trade_version)
start_events = {}

# key = (message_type, trade_id, trade_version, log_date)
durations = defaultdict(list)

# key = (message_type, log_date)
counts = defaultdict(int)

# =====================================================
# HELPERS
# =====================================================
def parse_datetime(date_str, time_str):
    """Parse datetime from log columns"""
    return datetime.strptime(
        f"{date_str} {time_str}", "%Y-%m-%d %H:%M:%S.%f"
    )

# =====================================================
# MAIN PROCESSING
# =====================================================
files = glob.glob(os.path.join(LOG_DIR, FILE_PATTERN))

if not files:
    logger.warning("No log files found")
    exit(0)

for file_path in files:
    logger.info(f"Processing file: {file_path}")

    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            parts = line.strip().split("|")
            if len(parts) < 7:
                continue

            log_date, log_time = parts[0], parts[1]
            message = parts[6]

            try:
                timestamp = parse_datetime(log_date, log_time)
            except ValueError:
                continue

            # ---------------- START ----------------
            start_match = START_PATTERN.search(message)
            if start_match:
                key = (
                    start_match.group("msg_type").strip(),
                    start_match.group("trade_id"),
                    start_match.group("trade_ver")
                )
                start_events[key] = (timestamp, log_date)
                continue

            # ---------------- END ----------------
            end_match = END_PATTERN.search(message)
            if end_match:
                key = (
                    end_match.group("msg_type").strip(),
                    end_match.group("trade_id"),
                    end_match.group("trade_ver")
                )

                if key in start_events:
                    start_time, start_date = start_events.pop(key)
                    duration_sec = (timestamp - start_time).total_seconds()

                    durations[(key[0], key[1], key[2], start_date)].append(duration_sec)
                    counts[(key[0], start_date)] += 1

# =====================================================
# PROMETHEUS PUSH
# =====================================================
registry = CollectorRegistry()

duration_gauge = Gauge(
    "reg_reporting_processing_seconds",
    "Processing time per regulatory message",
    ["message_type", "trade_id", "trade_version", "log_date"],
    registry=registry
)

count_gauge = Gauge(
    "reg_reporting_message_count",
    "Total processed messages per type per day",
    ["message_type", "log_date"],
    registry=registry
)

# ---------------- Populate duration gauge ----------------
for (msg_type, trade_id, trade_ver, log_date), durs in durations.items():
    avg_duration = sum(durs) / len(durs)
    duration_gauge.labels(
        message_type=msg_type,
        trade_id=trade_id,
        trade_version=trade_ver,
        log_date=log_date
    ).set(avg_duration)

# ---------------- Populate count gauge ----------------
for (msg_type, log_date), cnt in counts.items():
    count_gauge.labels(
        message_type=msg_type,
        log_date=log_date
    ).set(cnt)

push_to_gateway(
    PUSHGATEWAY_URL,
    job=JOB_NAME,
    registry=registry
)

logger.info("Metrics successfully pushed to Prometheus Pushgateway")