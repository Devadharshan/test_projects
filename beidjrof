import os
import re
from glob import glob
from datetime import datetime
from collections import deque
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway, pushadd_to_gateway

# ============================
# CONFIG
# ============================
LOG_DIR = r"C:\path\to\logs"
PUSHGATEWAY_URL = "http://localhost:9091"
JOB_NAME = "log_metrics_job"

DEBUG = True          # ENABLE LOGGING
MAX_WARN = 3600       # Warn if trade takes > 1 hour


# ============================
# REGEX PATTERNS
# ============================
start_pattern = re.compile(
    r"Found\s+MQ\s+M(?:esaage|essage)\s+ID\s*\[Hex\]\s*=\s*\[(.*?)\]",
    re.IGNORECASE
)

end_pattern = re.compile(
    r"Save trade \[(.*?)\] complete",
    re.IGNORECASE
)

failed_mq_pattern = re.compile(
    r"Failed\s+MQ(?:\s+M(?:esaage|message)\s+ID\s*\[.*?\])?",
    re.IGNORECASE
)

qproxy_request_pattern = re.compile(
    r"Received RequestManagerCall response from QProxy for Requestid\s+(\d+)\s+in\s+([\d.]+)\s*s",
    re.IGNORECASE
)

pv01_pattern = re.compile(
    r"Received PV01 response from QProxy for\s+(\d+)\s+with Request Id\s+(\d+)\s+in\s+([\d\.]+)\s*s",
    re.IGNORECASE
)

# Extract name + date from file name
file_regex = re.compile(r"^(AppFIMLImporter\d*)_.*__([\d\-]+)\.log$")


# ============================
# HELPERS
# ============================
def log(msg):
    if DEBUG:
        print(msg)


def parse_line_ts(parts):
    """Convert timestamp from log line."""
    if len(parts) < 2:
        return None
    dt_str = f"{parts[0].strip()} {parts[1].strip()}"
    try:
        return datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f")
    except:
        return None


def extract_name_from_filename(base):
    """Extract importer name + log date."""
    m = file_regex.match(base)
    if m:
        return m.group(1), m.group(2)
    return None, None


# ============================
# PROCESS ALL FILES
# ============================
file_results = {}
log_files = glob(os.path.join(LOG_DIR, "AppFIMLImporter*_ProdLon_x64__*.log"))

log(f"Found {len(log_files)} log files")

for log_file in log_files:

    base = os.path.basename(log_file)
    name_label, file_date = extract_name_from_filename(base)

    if not name_label:
        log(f"[SKIP] File did not match regex: {base}")
        continue

    log(f"\n--- Processing file: {base} ---")
    log(f"Importer Name: {name_label}, Log Date: {file_date}")

    start_stack = deque()
    active_trade_id = None

    trades = []
    qproxy_calls = []
    pv01_calls = []

    mq_total = 0
    mq_failed = 0

    processed_lines = 0

    with open(log_file, "r", errors="ignore") as fh:
        for raw in fh:
            processed_lines += 1

            parts = raw.rstrip("\n").split("|")
            ts = parse_line_ts(parts)
            if ts is None:
                continue

            msg = parts[6].strip() if len(parts) >= 7 else ""

            # ============================
            # START EVENT
            # ============================
            m = start_pattern.search(msg)
            if m:
                mq_total += 1
                start_stack.append((ts, m.group(1), None))
                log(f"  [+] START MQ MessageID={m.group(1)} at {ts}")
                continue

            # FAILED MQ
            if failed_mq_pattern.search(msg):
                mq_failed += 1
                log("  [!] FAILED MQ detected")

            # ============================
            # QPROXY REQUEST
            # ============================
            m = qproxy_request_pattern.search(msg)
            if m:
                req_id = m.group(1)
                duration = float(m.group(2))
                qproxy_calls.append({
                    "request_id": req_id,
                    "duration": duration,
                    "trade_id": active_trade_id
                })
                log(f"  [QPR] ReqID={req_id}, Duration={duration}, Trade={active_trade_id}")
                continue

            # ============================
            # PV01
            # ============================
            m = pv01_pattern.search(msg)
            if m:
                v = m.group(1)
                req_id = m.group(2)
                duration = float(m.group(3))
                pv01_calls.append({
                    "value": v,
                    "request_id": req_id,
                    "duration": duration,
                    "trade_id": active_trade_id
                })
                log(f"  [PV01] Value={v}, ReqID={req_id}, Duration={duration}, Trade={active_trade_id}")
                continue

            # ============================
            # END EVENT
            # ============================
            m = end_pattern.search(msg)
            if m:
                trade_id = m.group(1)
                active_trade_id = trade_id

                if start_stack:
                    start_ts, msg_id, _ = start_stack.pop()
                    duration = (ts - start_ts).total_seconds()

                    log(f"  [-] END Trade={trade_id} Duration={duration}s")

                    if duration < 0:
                        log("  [ERR] Negative duration — skipped")
                        continue

                    trades.append({
                        "message_id": msg_id,
                        "trade_id": trade_id,
                        "start": start_ts,
                        "end": ts,
                        "duration": duration
                    })

    log(f"Processed {processed_lines} lines")
    log(f"Trades: {len(trades)}, QProxy: {len(qproxy_calls)}, PV01: {len(pv01_calls)}")

    file_results[base] = {
        "name": name_label,
        "file_date": file_date,
        "mq_total": mq_total,
        "mq_failed": mq_failed,
        "trades": trades,
        "qproxy": qproxy_calls,
        "pv01": pv01_calls
    }


# ============================
# METRICS
# ============================
registry = CollectorRegistry()

labels = ["name", "file_name", "file_date"]

g_mq_total = Gauge("log_mq_messages_total", "Total MQ messages", labels, registry=registry)
g_mq_failed = Gauge("log_failed_mq_messages_total", "Total failed MQ messages", labels, registry=registry)
g_trades_processed = Gauge("log_trades_processed_total", "Trades processed", labels, registry=registry)

g_trade_duration = Gauge(
    "log_trade_duration_seconds",
    "Duration between MQ found and Save trade",
    ["name", "file_name", "file_date", "trade_id", "message_id"],
    registry=registry
)

g_qproxy = Gauge(
    "log_qproxy_request_time_seconds",
    "QProxy RequestManagerCall time",
    ["name", "file_name", "file_date", "request_id", "trade_id"],
    registry=registry
)

g_pv01 = Gauge(
    "log_qproxy_pv01_time_seconds",
    "QProxy PV01 time",
    ["name", "file_name", "file_date", "value", "request_id", "trade_id"],
    registry=registry
)


# ============================
# FILL METRICS
# ============================
have_data = False

for fname, data in file_results.items():

    if data["mq_total"] or data["mq_failed"] or len(data["trades"]) > 0:
        have_data = True

    name_label = data["name"]
    file_date = data["file_date"]

    g_mq_total.labels(name_label, fname, file_date).set(data["mq_total"])
    g_mq_failed.labels(name_label, fname, file_date).set(data["mq_failed"])
    g_trades_processed.labels(name_label, fname, file_date).set(len(data["trades"]))

    for t in data["trades"]:
        g_trade_duration.labels(
            name_label, fname, file_date, t["trade_id"], t["message_id"]
        ).set(t["duration"])

    for q in data["qproxy"]:
        g_qproxy.labels(
            name_label, fname, file_date, q["request_id"], q["trade_id"]
        ).set(q["duration"])

    for p in data["pv01"]:
        g_pv01.labels(
            name_label, fname, file_date,
            p["value"], p["request_id"], p["trade_id"]
        ).set(p["duration"])


# ============================
# PUSH METRICS ONLY IF DATA EXISTS
# ============================
if not have_data:
    print("No metrics found — nothing pushed.")
else:
    try:
        pushadd_to_gateway(PUSHGATEWAY_URL, job=JOB_NAME, registry=registry)
        print("Metrics pushed successfully.")
    except:
        push_to_gateway(PUSHGATEWAY_URL, job=JOB_NAME, registry=registry)
        print("Metrics pushed (fallback).")